---
content_type: page
is_media_gallery: true
title: Unit 5. Vision and Language
uid: 8d2bd38c-682c-4509-eb22-aad86d4047ab
---

Unit Overview
-------------

| ![Still photo of a tall blue bin in center, with a man to the left standing with a folding chair and a man to the right with a backpack.](BASEURL_PLACEHOLDER/resources/unit5) |  {{< br >}}{{< br >}} How do we recognize physical events in a dynamic visual scene? Andrei Barbu and his colleagues have developed a system that can generate a sentence like "The person to the right of the bin picked up the backpack" from a video clip portraying this action. {{< br >}}{{< br >}} (Image © Journal of Artificial Intelligence Research. All rights reserved. [This content is excluded](/help/faq-fair-use/) from our Creative Commons license. Source: Yu, H., N. Siddharth, A. Barbu, and J. M. Siskind. "A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video." _J. Artif. Intell. Res. (JAIR)_ 52 (2015): 601-713.) {{< br >}}{{< br >}}  

The ability to obtain and communicate complex knowledge about a visual scene, in order to answer simple questions about the objects, agents, and actions portrayed, requires the integration of vision with language understanding. In this unit, you will learn about the state-of-the-art in automated question answering systems; models that leverage visual recognition and tracking with language understanding to describe the content of a video in linguistic terms; and a system that can understand stories. Turning to biology, you will learn about the representations of semantic information in the brain as revealed by fMRI studies.

Boris Katz describes key elements of the START system, an online question answering system that has been operating for over two decades, and compares its capabilities to IBM's Watson system that can beat human players at Jeopardy.

Andrei Barbu shows how the simple ability to compare an English sentence and a video clip can form the basis for many tasks such as recognition, image and video retrieval, generation of video captions, question answering, and language acquisition.

Patrick Winston addresses a cognitive ability that distinguishes human intelligence from that of other primates: The ability to tell, understand, and recombine stories. The Genesis story understanding system is a powerful and flexible platform for exploring this capability.

Guest speaker Tom Mitchell shows how the neural representations of language meaning can be understood using machine learning methods that can decode fMRI signals to reveal the semantics of words experienced by a viewer.

Unit Activities
---------------

### Useful Background

*   Introductions to machine learning, neuroscience

### Videos and Slides{{< video-gallery-item href="/resources/lecture-5" section="Unit 5. Vision and Language" title="Lecture 5.1: Boris Katz - Vision and Language" description="Description: Combining language and vision processing to solve problems in computer scene recognition and scene understanding, language understanding and knowledge representation in the START question answering system, comparison to IBM’s Watson. Instructor: Boris Katz" thumbnail="https://img.youtube.com/vi/yEbr410E2RE/default.jpg" >}} {{< video-gallery-item href="/resources/lecture-5-1" section="Unit 5. Vision and Language" title="Lecture 5.2: Andrei Barbu - From Language to Vision and Back Again" description="Description: Using higher level knowledge to improve object detection, language-vision model that simultaneously processes sentences and recognizes image objects and events, performing tasks like image/video retrieval, generating descriptions, and question answering. Instructor: Andrei Barbu" thumbnail="https://img.youtube.com/vi/NRygklHAoEw/default.jpg" >}} {{< video-gallery-item href="/resources/lecture-5-2" section="Unit 5. Vision and Language" title="Lecture 5.3: Patrick Winston - Story Understanding" description="Description: The strong story hypothesis that the ability to tell, understand, and recombine stories distinguishes human and primate intelligence, historical perspective on AI and thinking machines, modelling story understanding in the Genesis system. Instructor: Patrick Winston" thumbnail="https://img.youtube.com/vi/7XvgBI2KV28/default.jpg" >}} {{< video-gallery-item href="/resources/seminar-5-tom-mitchell-neural-representations-of-language" section="Unit 5. Vision and Language" title="Seminar 5: Tom Mitchell - Neural Representations of Language" description="Description: Modelling the neural representations of language using machine learning to classify words from fMRI data, predictive models for word feature combinations, probing the timing of semantic processing with MEG, neural interpretation of adjective-noun phrases. Instructor: Tom Mitchell" thumbnail="https://img.youtube.com/vi/A4R2PQOHT2w/default.jpg" >}}
Further Study
-------------

Additional information about the speakers' research and publications can be found at their websites:

*   [Boris Katz, MIT](http://people.csail.mit.edu/boris/boris.html)
*   [Andrei Barbu, MIT](http://0xab.com/)
*   [Patrick Winston, MIT](http://people.csail.mit.edu/phw/index.html)
*   [Tom Mitchell, CMU](http://www.cs.cmu.edu/~tom/)

Berzak, Y., A. Barbu, et al. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["Do You See What I Mean? Visual Resolution of Linguistic Ambiguities." (PDF - 2.4MB)](http://start.csail.mit.edu/publications/EMNLP172.pdf) _Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing_ (2015): 1477–87.

Huth, A. G., S. Nishimoto, et al. "[A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories Across the Human Brain](http://dx.doi.org/10.1016/j.neuron.2012.10.014)." _Neuron_ 76, no. 6 (2012): 1210–24.

Katz, B. "[START Natural Language Question Answering System](http://start.csail.mit.edu/index.php). " (online resource)

Mitchell, T., S. V. Shinkareva, et al. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["Predicting Human Brain Activity Associated with the Meanings of Nouns." (PDF)](http://www.cs.cmu.edu/~tom/pubs/science2008.pdf) _Science_ 320 (2008): 1191–95.

Siddharth, N., A. Barbu, et al. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["Seeing What You're Told: Sentence-Guided Activity Recognition in Video." (PDF)](http://0xab.com/papers/cvpr2014.pdf) _IEEE Conference on Computer Vision and Pattern Recognition_ (2014).

Sudre, G., D. Pomerleau, et al. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["Tracking Neural Coding of Perceptual and Semantic Features of Concrete Nouns." (PDF - 1.3MB)](http://www.cs.cmu.edu/~tom/pubs/sudre_2012.pdf) _NeuroImage_ 62 (2012): 451–63.

Wehbe, L., B. Murphy, et al. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["Simultaneously Uncovering the Patterns of Brain Regions Involved in Different Story Reading Subprocesses." (PDF - 1.1MB)](http://journals.plos.org/plosone/article/asset?id=10.1371%2Fjournal.pone.0112575.PDF) _PLOS One_ (2014): 1–19.

Winston, P. H. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["The Genesis Story Understanding and Story Telling System: A 21st Century Step toward Artificial Intelligence." (PDF)](http://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-019_StoryWhitePaper.pdf) _Center for Barins, Minds & Machines,_ Memo no. 019 (2014).

———. "[The Right Way.](http://dspace.mit.edu/handle/1721.1/72174)" _Advances in Cognitive Systems_ 1 (2012): 23–36.

Yu, H., N. Siddharth, et al. ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)["A Compositional Framework for Grounded Language Inference, Generation, and Acquisition in Video."](http://upplysingaoflun.ecn.purdue.edu/~qobi/cccp/grounding-language-in-video.html)  _Journal of Artificial Intelligence Research_ 52 (2015): 601–713.